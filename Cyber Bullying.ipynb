{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import tflearn\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import preprocessor as p\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from tensorflow.contrib import learn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from scipy import stats\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_1d, global_max_pool\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.estimator import regression\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional\n",
    "from keras.models import Model,Sequential\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These Functions are for loading the data\n",
    "def load_data(filename):\n",
    "    print(\"Loading data from file: \" + filename)\n",
    "    data = pickle.load(open(filename, 'rb'))\n",
    "    x_text = []\n",
    "    labels = [] \n",
    "    for i in range(len(data)):\n",
    "        x_text.append(p.tokenize((data[i]['text'])))\n",
    "        labels.append(data[i]['label'])\n",
    "    return x_text,labels\n",
    "\n",
    "def get_filename(dataset):\n",
    "    global HASH_REMOVE\n",
    "    if(dataset==\"twitter\"):\n",
    "        HASH_REMOVE = True\n",
    "        EPOCHS = 10\n",
    "        BATCH_SIZE = 128\n",
    "        MAX_FEATURES = 2\n",
    "        filename = \"/home/yogesh/Desktop/Minor/data/twitter_data.pkl\"\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = \"twitter\"\n",
    "data_2 = \"wiki\"\n",
    "model_type =\"blstm_attention\"\n",
    "vector_type = \"sswe\"\n",
    "embed_size = 50\n",
    "oversampling_rate = 3\n",
    "max_document_length=None\n",
    "EMBED_SIZE = 50\n",
    "EPOCHS = 8\n",
    "BATCH_SIZE = 128\n",
    "MAX_FEATURES = 2\n",
    "NUM_CLASSES = 2\n",
    "DROPOUT = 0.25\n",
    "LEARN_RATE = 0.01\n",
    "#HASH_REMOVE=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file: /home/yogesh/Desktop/Minor/data/twitter_data.pkl\n",
      "Counter after oversampling\n",
      "Counter({1: 15162, 0: 11036})\n"
     ]
    }
   ],
   "source": [
    "x_text, labels = load_data(get_filename(data_1))\n",
    "dict1 = {'racism':1,'sexism':1,'none':0} #Transfer learning only two classes\n",
    "labels = [dict1[b] for b in labels]        \n",
    "racism = [i for i in range(len(labels)) if labels[i]==2]\n",
    "sexism = [i for i in range(len(labels)) if labels[i]==1]\n",
    "x_text = x_text + [x_text[x] for x in racism]*(oversampling_rate-1)+ [x_text[x] for x in sexism]*(oversampling_rate-1)\n",
    "labels = labels + [2 for i in range(len(racism))]*(oversampling_rate-1) + [1 for i in range(len(sexism))]*(oversampling_rate-1)\n",
    "print(\"Counter after oversampling\")\n",
    "from collections import Counter\n",
    "print(Counter(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into training set and testing set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( x_text, labels, random_state=121, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"rt $MENTION$: that kitchen looks like they have re-enacted '2 girls , $NUMBER$ cup'. $HASHTAG$\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document length : 38\n"
     ]
    }
   ],
   "source": [
    "post_length = post_length = np.array([len(x.split(\" \")) for x in x_text])\n",
    "if(data_1 != \"twitter\"):\n",
    "    max_document_length = int(np.percentile(post_length, 95))\n",
    "else:\n",
    "    max_document_length = max(post_length)\n",
    "print(\"Document length : \" + str(max_document_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-85a95e093a35>:1: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /home/yogesh/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /home/yogesh/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "Vocabulary Size: 8894\n"
     ]
    }
   ],
   "source": [
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length, MAX_FEATURES)\n",
    "vocab_processor = vocab_processor.fit(x_text)\n",
    "trainX = np.array(list(vocab_processor.transform(X_train)))\n",
    "testX = np.array(list(vocab_processor.transform(X_test)))\n",
    "\n",
    "vocab_size = len(vocab_processor.vocabulary_)\n",
    "print(\"Vocabulary Size: {:d}\".format(vocab_size))\n",
    "\n",
    "vocab = vocab_processor.vocabulary_._mapping\n",
    "trainY = np.asarray(Y_train)\n",
    "testY = np.asarray(Y_test)\n",
    "       \n",
    "trainX = pad_sequences(trainX, maxlen=max_document_length, value=0.)\n",
    "testX = pad_sequences(testX, maxlen=max_document_length, value=0.)\n",
    "trainY = to_categorical(trainY, nb_classes=NUM_CLASSES)\n",
    "testY = to_categorical(testY, nb_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model: blstm_attention with word vector initiliazed with sswe word vectors.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 38, 50)            444700    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 38, 50)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 38, 100)           40400     \n",
      "_________________________________________________________________\n",
      "att_layer_1 (AttLayer)       (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 485,402\n",
      "Trainable params: 485,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Running Model: \" + model_type + \" with word vector initiliazed with \" + vector_type + \" word vectors.\")\n",
    "\n",
    "#Attention Layer Class for blstm attention model\n",
    "class AttLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[-1],),\n",
    "                                      initializer='random_normal',\n",
    "                                      trainable=True)\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        \n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_size, input_length=trainX.shape[1]))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Bidirectional(LSTM(embed_size, return_sequences=True)))\n",
    "model.add(AttLayer())\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "adam = optimizers.Adam(lr=LEARN_RATE, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vectors used: sswe\n",
      "Loaded from file: /home/yogesh/Desktop/Minor/embedding-results/sswe-u.txt\n",
      "1159 embedding missed  of  8894\n",
      "Epoch 1/8\n",
      "23578/23578 [==============================] - 34s 1ms/step - loss: 0.5874 - acc: 0.6813\n",
      "Epoch 2/8\n",
      "23578/23578 [==============================] - 35s 1ms/step - loss: 0.4760 - acc: 0.7673\n",
      "Epoch 3/8\n",
      "23578/23578 [==============================] - 36s 2ms/step - loss: 0.3774 - acc: 0.8318\n",
      "Epoch 4/8\n",
      "23578/23578 [==============================] - 37s 2ms/step - loss: 0.3102 - acc: 0.8709\n",
      "Epoch 5/8\n",
      "23578/23578 [==============================] - 37s 2ms/step - loss: 0.2694 - acc: 0.8894\n",
      "Epoch 6/8\n",
      "23578/23578 [==============================] - 37s 2ms/step - loss: 0.2467 - acc: 0.9008\n",
      "Epoch 7/8\n",
      "23578/23578 [==============================] - 37s 2ms/step - loss: 0.2239 - acc: 0.9117\n",
      "Epoch 8/8\n",
      "23578/23578 [==============================] - 39s 2ms/step - loss: 0.2052 - acc: 0.9194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f90f87ce048>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Word vectors used: \" + vector_type)\n",
    "def get_embedding_weights(filename, sep):\n",
    "    embed_dict = {}\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(sep)\n",
    "        embed_dict[row[0]] = row[1:]\n",
    "    print('Loaded from file: ' + str(filename))\n",
    "    file.close()\n",
    "    return embed_dict\n",
    "\n",
    "def map_embedding_weights(embed, vocab, embed_size):\n",
    "    vocab_size = len(vocab)\n",
    "    embeddingWeights = np.zeros((vocab_size , embed_size))\n",
    "    n = 0\n",
    "    words_missed = []\n",
    "    for k, v in vocab.items():\n",
    "        try:\n",
    "            embeddingWeights[v] = embed[k]\n",
    "        except:\n",
    "            n += 1\n",
    "            words_missed.append(k)\n",
    "            pass\n",
    "    print(\"%d embedding missed\"%n, \" of \" , vocab_size)\n",
    "    return embeddingWeights\n",
    "\n",
    "def get_embeddings_dict(vector_type, emb_dim, data):\n",
    "    if vector_type == 'sswe':\n",
    "        emb_dim==50\n",
    "        sep = '\\t'\n",
    "        vector_file = '/home/yogesh/Desktop/Minor/embedding-results/sswe-u.txt'\n",
    "    else:\n",
    "        print (\"ERROR: Please specify a correst model or SSWE cannot be loaded with embed size of: \" + str(emb_dim))\n",
    "        return None\n",
    "    \n",
    "    embed = get_embedding_weights(vector_file, sep)\n",
    "    return embed\n",
    "#TRAINING THE MODEL\n",
    "model.layers[0].set_weights([map_embedding_weights(get_embeddings_dict(vector_type, embed_size, data_1), vocab, embed_size)])\n",
    "model.fit(trainX, trainY, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(testX, testY, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 91.79%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Successfully Saved To Disk!\n"
     ]
    }
   ],
   "source": [
    "model.save_weights(\"bully.h5\")\n",
    "print(\"Model Successfully Saved To Disk!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  10,    1,    3,    3,  633, 8074,   80,    2, 2029, 8599, 2401,\n",
       "        288,   26,    7, 2633,  180,    9,   68,   52,   27,  474,    6,\n",
       "        105,   83,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to check model on test data\n",
    "temp = model.predict(testX)\n",
    "y_pred  = np.argmax(temp, 1)\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i]==1:\n",
    "        print(\"Racism or Sexism!\")\n",
    "    else :\n",
    "        print(\"None!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "Sexist or Racist Comment!\n"
     ]
    }
   ],
   "source": [
    "tweet1='rt $MENTION$: $HASHTAG$ $HASHTAG$ model ? puhlease. did the local thrift shop put on a \"fashion\" show. and should we be able to see her…'\n",
    "tokens=p.tokenize(tweet1)\n",
    "l=[]\n",
    "l.append(tokens)\n",
    "arr=np.array(list(vocab_processor.transform(l)))\n",
    "res=np.argmax(model.predict(arr),1)\n",
    "print(res)\n",
    "if res[0]==1:\n",
    "    print(\"Sexist or Racist Comment!\")\n",
    "else :\n",
    "    print(\"None!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
